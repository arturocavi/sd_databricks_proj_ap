{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f53e1f3b-9f25-4fef-8726-fa3a7881b092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingestion Explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce16dd6-5fbb-4c60-87b0-3a2c92791080",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764886909729}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading data from the container to check the schema\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"abfss://source@sd0212.dfs.core.windows.net/ap_invoices\")\n",
    ")\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594e97b2-17b4-4c9a-9ba4-b940ade9fd69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading data from the container to check the schema\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"abfss://source@sd0212.dfs.core.windows.net/suppliers\")\n",
    ")\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3178f677-928d-45c3-97ac-086bfa4e43e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading data from the container to check the schema\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"abfss://source@sd0212.dfs.core.windows.net/gl_control_totals\")\n",
    ")\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e195c7-248b-48d5-9bbe-4cf2a26e871e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a6950b-f18a-4149-8d2a-496a55a53a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading data from the container to check the schema\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"abfss://source@sd0212.dfs.core.windows.net/ap_invoices\")\n",
    ")\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6aa1b35-97f3-4f9f-9e7c-0b6604a52636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0978ed9-61c6-484e-b3e9-10cbb5a0f0f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Reading Module\n",
    "===================================\n",
    "\"\"\"\n",
    "# ============================================================================\n",
    "# DEPENDENCIES\n",
    "# ============================================================================\n",
    "import logging\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Storage configuration\n",
    "STORAGE_ACCOUNT = \"sd0212\"\n",
    "CHECKPOINT_CONTAINER = \"bronze\"\n",
    "SOURCE_CONTAINER = \"source\"\n",
    "\n",
    "# Tables configuration\n",
    "TABLES = [\"ap_invoices\", \"suppliers\", \"gl_control_totals\"]\n",
    "SCHEMA_HINTS = {\n",
    "    \"ap_invoices\": \"InvoiceID STRING, InvoiceDate STRING, DueDate STRING, PaidDate STRING, SupplierID INT, Category STRING, CostCenter STRING, InvoiceAmount DOUBLE, Currency STRING, POID STRING, Quantity INT, UnitPrice_PO DOUBLE, UnitPrice_Invoice DOUBLE\",\n",
    "    \"suppliers\": \"SupplierID INT, Supplier STRING\",\n",
    "    \"gl_control_totals\": \"GL_Approved_Spend DOUBLE, Month STRING\"\n",
    "}\n",
    "\n",
    "FORMATS = {\n",
    "    \"ap_invoices\": \"csv\",\n",
    "    \"suppliers\": \"csv\",\n",
    "    \"gl_control_totals\": \"json\"\n",
    "}\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s'\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION\n",
    "# ============================================================================\n",
    "def auto_load_data(storage_account, checkpoint_container, source_container, table, file_format, schema_hints):\n",
    "    \"\"\"\n",
    "    Auto-load data from source container to bronze container using Structured Streaming.\n",
    "    \n",
    "    Args:\n",
    "        storage_account: Azure storage account name\n",
    "        checkpoint_container: Container for checkpoints and output data\n",
    "        source_container: Container with source data\n",
    "        table: Table name to process\n",
    "        schema_hints: Dictionary with schema hints for each table\n",
    "    \n",
    "    Returns:\n",
    "        StreamingQuery: The streaming query object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        checkpoint_path = f\"abfss://{checkpoint_container}@{storage_account}.dfs.core.windows.net/checkpoint_{table}\"\n",
    "        source_path = f\"abfss://{source_container}@{storage_account}.dfs.core.windows.net/{table}\"\n",
    "        output_path = f\"abfss://{checkpoint_container}@{storage_account}.dfs.core.windows.net/{table}\"\n",
    "        \n",
    "        # Read stream with Auto Loader\n",
    "        df = (\n",
    "            spark\n",
    "            .readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", file_format)\n",
    "            .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "            .option(\"cloudFiles.schemaHints\", schema_hints[table])\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .load(source_path)\n",
    "        )   \n",
    "        \n",
    "        # Write stream \n",
    "        query = (\n",
    "            df\n",
    "            .writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", checkpoint_path)\n",
    "            .option(\"path\", output_path)\n",
    "            .trigger(availableNow=True)\n",
    "            .start()\n",
    "        )\n",
    "        \n",
    "        # Wait for the stream to complete (since using availableNow trigger)\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        logging.info(f\"Data loading for table {table} completed successfully.\")\n",
    "        return query\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing table {table}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "for table in TABLES:\n",
    "    try:\n",
    "        auto_load_data(STORAGE_ACCOUNT, CHECKPOINT_CONTAINER, SOURCE_CONTAINER, table, FORMATS, SCHEMA_HINTS)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process table {table}: {str(e)}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion_exp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
