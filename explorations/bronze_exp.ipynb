{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a5d065-6776-4c36-83c0-032d61ce267b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dcb1c21-7be1-48bf-a9c7-37381873ca21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a7120d5-b28a-47d2-9ee5-948e6b3c2334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, to_timestamp_ntz \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1339ce1f-02bb-4992-96ef-45aa6bfd4aa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "suppliers_schema = StructType([\n",
    "    StructField('SupplierID', IntegerType(), True), \n",
    "    StructField('Supplier', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d8cc40-3a71-400c-984c-58f98c3bab20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Storage configuration\n",
    "STORAGE_ACCOUNT = \"sd0212\"\n",
    "CONTAINER = \"bronze\"\n",
    "CATALOG = \"ap\"\n",
    "SCHEMA = \"bronze\"\n",
    "TABLE = \"suppliers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5690b4b8-6794-49a6-9e68-a4de4f9474b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to storage\n",
    "path = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{TABLE}\"\n",
    "df = spark.read.format('delta').load(path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee3ca8f-bb3e-4908-9f3a-fd686249447c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ingest_time\":227},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764896256236}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to storage\n",
    "path = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{TABLE}\"\n",
    "\n",
    "# Read data from storage\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .option(\"header\", True)\n",
    "    .load(path)\n",
    "    .withColumn(\"ingest_time\", to_timestamp_ntz(current_timestamp()))\n",
    "    .drop(\"_rescued_data\")\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f6eee0-062f-44c4-a14e-ce21269dd5ad",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764896546346}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Renaming columns to lowercase\n",
    "df = (\n",
    "    df\n",
    "    .toDF(*[c.lower() for c in df.columns])\n",
    "    .withColumnRenamed(\"supplierid\", \"supplier_id\")\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b1c797-3860-44dd-bb90-b25cd90192b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data to table\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{CATALOG}.{SCHEMA}.{TABLE}\")\n",
    ")\n",
    "\n",
    "spark.read.table(f\"{CATALOG}.{SCHEMA}.{TABLE}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5cb1891-f6d9-4c56-b63f-6de0015af185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Python Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d972964-f091-42a8-99af-29ec70c3b361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28753a90-903f-43d4-9dc4-daaa672472f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bronze layer - Suppliers\n",
    "===================================\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# DEPENDENCIES\n",
    "# ============================================================================\n",
    "from pyspark.sql.functions import current_timestamp, to_timestamp_ntz \n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Storage configuration\n",
    "STORAGE_ACCOUNT = \"sd0212\"\n",
    "CONTAINER = \"bronze\"\n",
    "CATALOG = \"ap\"\n",
    "SCHEMA = \"bronze\"\n",
    "TABLE = \"suppliers\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE CREATION\n",
    "# ============================================================================\n",
    "# Path to storage\n",
    "path = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{TABLE}\"\n",
    "\n",
    "# Read data from storage\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .option(\"header\", True)\n",
    "    .load(path)\n",
    "    .withColumn(\"ingest_time\", to_timestamp_ntz(current_timestamp()))\n",
    "    .drop(\"_rescued_data\")\n",
    ")\n",
    "\n",
    "# Renaming columns to lowercase\n",
    "df = (\n",
    "    df\n",
    "    .toDF(*[c.lower() for c in df.columns])\n",
    "    .withColumnRenamed(\"supplierid\", \"supplier_id\")\n",
    ")\n",
    "\n",
    "# Write data to table\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{CATALOG}.{SCHEMA}.{TABLE}\")\n",
    ")\n",
    "\n",
    "\n",
    "# Display table\n",
    "spark.read.table(f\"{CATALOG}.{SCHEMA}.{TABLE}\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2606b377-d67b-4d35-8fb8-476358576f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bronze layer - GL Control Totals\n",
    "===================================\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# DEPENDENCIES\n",
    "# ============================================================================\n",
    "from pyspark.sql.functions import current_timestamp, to_timestamp_ntz \n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Storage configuration\n",
    "STORAGE_ACCOUNT = \"sd0212\"\n",
    "CONTAINER = \"bronze\"\n",
    "CATALOG = \"ap\"\n",
    "SCHEMA = \"bronze\"\n",
    "TABLE = \"gl_control_totals\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE CREATION\n",
    "# ============================================================================\n",
    "# Path to storage\n",
    "path = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{TABLE}\"\n",
    "\n",
    "# Read data from storage\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .option(\"header\", True)\n",
    "    .load(path)\n",
    "    .withColumn(\"ingest_time\", to_timestamp_ntz(current_timestamp()))\n",
    "    .drop(\"_rescued_data\")\n",
    ")\n",
    "\n",
    "# Renaming columns to lowercase\n",
    "df = (\n",
    "    df\n",
    "    .toDF(*[c.lower() for c in df.columns])\n",
    ")\n",
    "\n",
    "# Write data to table\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{CATALOG}.{SCHEMA}.{TABLE}\")\n",
    ")\n",
    "\n",
    "\n",
    "# Display table\n",
    "spark.read.table(f\"{CATALOG}.{SCHEMA}.{TABLE}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c98693e-e275-4ea2-a7a0-edd726ac4ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### AP Invoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99269460-966c-4706-89b1-f2fe648a8a26",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764898687721}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bronze layer - AP Invoices\n",
    "===================================\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# DEPENDENCIES\n",
    "# ============================================================================\n",
    "from pyspark.sql.functions import current_timestamp, to_timestamp_ntz \n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Storage configuration\n",
    "STORAGE_ACCOUNT = \"sd0212\"\n",
    "CONTAINER = \"bronze\"\n",
    "CATALOG = \"ap\"\n",
    "SCHEMA = \"bronze\"\n",
    "TABLE = \"ap_invoices\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE CREATION\n",
    "# ============================================================================\n",
    "# Path to storage\n",
    "path = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{TABLE}\"\n",
    "\n",
    "# Read data from storage\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .option(\"header\", True)\n",
    "    .load(path)\n",
    "    .withColumn(\"ingest_time\", to_timestamp_ntz(current_timestamp()))\n",
    "    .drop(\"_rescued_data\")\n",
    "    .drop(\"currency\")\n",
    ")\n",
    "\n",
    "# Renaming columns to lowercase\n",
    "df = (\n",
    "    df\n",
    "    .toDF(*[c.lower() for c in df.columns])\n",
    "    .withColumnRenamed(\"invoiceid\", \"invoice_id\")\n",
    "    .withColumnRenamed(\"invoicedate\", \"invoice_date\")\n",
    "    .withColumnRenamed(\"duedate\",\"due_date\")\n",
    "    .withColumnRenamed(\"paiddate\",\"paid_date\")\n",
    "    .withColumnRenamed(\"supplierid\", \"supplier_id\")\n",
    "    .withColumnRenamed(\"costcenter\", \"cost_center\")\n",
    "    .withColumnRenamed(\"invoiceamount\", \"invoice_amount\")\n",
    "    .withColumnRenamed(\"unitprice_po\", \"unit_price_po\")\n",
    "    .withColumnRenamed(\"unitprice_invoice\", \"unit_price_inv\")\n",
    ")\n",
    "\n",
    "# Write data to table\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{CATALOG}.{SCHEMA}.{TABLE}\")\n",
    ")\n",
    "\n",
    "\n",
    "# Display table\n",
    "spark.read.table(f\"{CATALOG}.{SCHEMA}.{TABLE}\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_exp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
