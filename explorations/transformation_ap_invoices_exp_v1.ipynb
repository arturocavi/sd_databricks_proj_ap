{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1a40f39-73f3-4154-9fe0-66c6cb70edde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transformation Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d19008-feb4-404c-ba3f-eff66e983670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Whole Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9ed0079-4835-4351-a6fe-ac0ae145c79b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa0ffca4-d49a-420e-b998-87138066c301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# ACCOUNTS PAYABLE INVOICES TRANSFORMATION PIPELINE\n",
    "# ====================================================\n",
    "\"\"\"\n",
    "This script performs data transformation on the 'ap.bronze.ap_invoices' table.\n",
    "\n",
    "Steps:\n",
    "    1. Standardize date formats\n",
    "    2. Clean text and ID fields\n",
    "    3. Create an enriched dataset with calculated fields:\n",
    "        - POAmount\n",
    "        - OnTimePayment\n",
    "        - AgingDays\n",
    "        - Month\n",
    "\n",
    "Assumptions:\n",
    "    - 'today' is the latest date found in 'InvoiceDate' or 'PaidDate'.\n",
    "    - If 'PaidDate' is blank, the invoice is considered unpaid.\n",
    "    - If unpaid and 'DueDate' > 'today', the invoice is not considered overdue.\n",
    "\"\"\"\n",
    "\n",
    "# --- DEPENDENCIES ---\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BooleanType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_TABLE = \"ap.bronze.ap_invoices\"\n",
    "OUTPUT_TABLE = \"ap.silver.ap_invoices_enriched\"  # Adjust as needed\n",
    "DATE_COLS = [\"InvoiceDate\", \"DueDate\", \"PaidDate\"]\n",
    "TEXT_FIELDS = [\"Supplier\", \"Category\"]\n",
    "ID_FIELDS = [\"InvoiceID\", \"CostCenter\", \"POID\", \"Currency\"]\n",
    "\n",
    "\n",
    "# --- FUNCTIONS ---\n",
    "def standardize_dates(df: DataFrame, columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert date columns to date type safely.\n",
    "    Tries multiple formats before falling back to default parsing.\n",
    "    \"\"\"\n",
    "    common_formats = [\n",
    "        \"MM/dd/yyyy\",  # 10/30/2025\n",
    "        \"MM/dd/yy\",    # 10/30/25\n",
    "        \"dd/MM/yyyy\",  # 30/10/2025\n",
    "        \"yyyy/MM/dd\",  # 2025/10/30\n",
    "        \"yyyy-MM-dd\"   # 2025-10-30\n",
    "    ]\n",
    "    \n",
    "    for col in columns:\n",
    "        # Try each format\n",
    "        date_col = None\n",
    "        for fmt in common_formats:\n",
    "            temp_col = F.to_date(F.col(col), fmt)\n",
    "            if date_col is None:\n",
    "                date_col = temp_col\n",
    "            else:\n",
    "                # Coalesce to try next format if previous failed\n",
    "                date_col = F.coalesce(date_col, temp_col)\n",
    "        \n",
    "        # Replace original column with parsed date\n",
    "        df = df.withColumn(col, date_col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text_fields(df: DataFrame, text_cols: list, id_cols: list) -> DataFrame:\n",
    "    \"\"\"Clean text and ID fields: trim spaces, normalize casing.\"\"\"\n",
    "    \n",
    "    # Clean text fields (trim and title case)\n",
    "    for col in text_cols:\n",
    "        df = df.withColumn(\n",
    "            col,\n",
    "            F.initcap(F.trim(F.col(col)))\n",
    "        )\n",
    "    \n",
    "    # Clean ID fields (trim and upper case)\n",
    "    for col in id_cols:\n",
    "        df = df.withColumn(\n",
    "            col,\n",
    "            F.upper(F.trim(F.col(col)))\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Add calculated columns: POAmount, OnTimePayment, AgingDays, Month.\"\"\"\n",
    "    \n",
    "    # Compute POAmount\n",
    "    df = df.withColumn(\"POAmount\", F.col(\"Quantity\") * F.col(\"UnitPrice_PO\"))\n",
    "    \n",
    "    # Define 'today' as the latest date from InvoiceDate or PaidDate\n",
    "    today = df.select(\n",
    "        F.greatest(\n",
    "            F.max(\"InvoiceDate\"),\n",
    "            F.max(\"PaidDate\")\n",
    "        ).alias(\"today\")\n",
    "    ).first()[\"today\"]\n",
    "    \n",
    "    # Create OnTimePayment column\n",
    "    df = df.withColumn(\n",
    "        \"OnTimePayment\",\n",
    "        F.when(\n",
    "            F.col(\"PaidDate\").isNotNull() & (F.col(\"PaidDate\") <= F.col(\"DueDate\")),\n",
    "            F.lit(True)\n",
    "        ).when(\n",
    "            F.col(\"PaidDate\").isNotNull() & (F.col(\"PaidDate\") > F.col(\"DueDate\")),\n",
    "            F.lit(False)\n",
    "        ).when(\n",
    "            F.col(\"PaidDate\").isNull() & (F.col(\"DueDate\") <= F.lit(today)),\n",
    "            F.lit(False)\n",
    "        ).otherwise(F.lit(None)).cast(BooleanType())\n",
    "    )\n",
    "    \n",
    "    # Create AgingDays column\n",
    "    df = df.withColumn(\n",
    "        \"AgingDays\",\n",
    "        F.when(\n",
    "            F.col(\"PaidDate\").isNull(),\n",
    "            F.datediff(F.lit(today), F.col(\"DueDate\"))\n",
    "        ).otherwise(\n",
    "            F.datediff(F.col(\"PaidDate\"), F.col(\"InvoiceDate\"))\n",
    "        ).cast(IntegerType())\n",
    "    )\n",
    "    \n",
    "    # Create Month column\n",
    "    df = df.withColumn(\n",
    "        \"Month\",\n",
    "        F.date_format(F.col(\"InvoiceDate\"), \"yyyy-MM\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# --- MAIN PIPELINE ---\n",
    "def main():\n",
    "    \"\"\"Execute the AP Invoices data transformation pipeline.\"\"\"\n",
    "    \n",
    "    # Get Spark session (already available in Databricks)\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    print(f\"Loading source data from {INPUT_TABLE}...\")\n",
    "    df = spark.table(INPUT_TABLE)\n",
    "    \n",
    "    print(\"Standardizing date formats...\")\n",
    "    df = standardize_dates(df, DATE_COLS)\n",
    "    \n",
    "    print(\"Cleaning text and ID fields...\")\n",
    "    df = clean_text_fields(df, TEXT_FIELDS, ID_FIELDS)\n",
    "    \n",
    "    print(\"Enriching data with calculated columns...\")\n",
    "    df = enrich_data(df)\n",
    "    \n",
    "    print(f\"Writing enriched data to {OUTPUT_TABLE}...\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_TABLE)\n",
    "    \n",
    "    print(\"Transformation pipeline completed successfully!\")\n",
    "    \n",
    "    # Return the DataFrame for further use if needed\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- RUN SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = main()\n",
    "    # Optionally display results in Databricks notebook\n",
    "    display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ace16caa-1746-450e-9403-5fc66b1bc274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48699084-eb4a-4513-8f0a-2154129a8de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Silver Layer - Accounts Payable Invoices Transformation Pipeline\n",
    "======================================================================\n",
    "This script performs data transformation on the 'ap.bronze.ap_invoices' table.\n",
    "\n",
    "Steps:\n",
    "    1. Standardize date formats\n",
    "    2. Clean text and ID fields\n",
    "    3. Create an enriched dataset with calculated fields:\n",
    "        - POAmount\n",
    "        - OnTimePayment\n",
    "        - AgingDays\n",
    "        - Month\n",
    "\n",
    "Assumptions:\n",
    "    - 'today' is the latest date found in 'InvoiceDate' or 'PaidDate'.\n",
    "    - If 'PaidDate' is blank, the invoice is considered unpaid.\n",
    "    - If unpaid and 'DueDate' > 'today', the invoice is not considered overdue.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# DEPENDENCIES\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import BooleanType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "INPUT_TABLE = \"ap.bronze.ap_invoices\"\n",
    "OUTPUT_TABLE = \"ap.silver.ap_invoices_enriched\"  # Adjust as needed\n",
    "DATE_COLS = [\"InvoiceDate\", \"DueDate\", \"PaidDate\"]\n",
    "TEXT_FIELDS = [\"Supplier\", \"Category\"]\n",
    "ID_FIELDS = [\"InvoiceID\", \"CostCenter\", \"POID\", \"Currency\"]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTIONS\n",
    "# ============================================================================\n",
    "def standardize_dates(df: DataFrame, columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert date columns to date type safely.\n",
    "    Tries multiple formats before falling back to default parsing.\n",
    "    \"\"\"\n",
    "    common_formats = [\n",
    "        \"MM/dd/yyyy\",  # 10/30/2025\n",
    "        \"MM/dd/yy\",    # 10/30/25\n",
    "        \"dd/MM/yyyy\",  # 30/10/2025\n",
    "        \"yyyy/MM/dd\",  # 2025/10/30\n",
    "        \"yyyy-MM-dd\"   # 2025-10-30\n",
    "    ]\n",
    "    \n",
    "    for col in columns:\n",
    "        # Try each format\n",
    "        date_col = None\n",
    "        for fmt in common_formats:\n",
    "            temp_col = to_date(col(col), fmt)\n",
    "            if date_col is None:\n",
    "                date_col = temp_col\n",
    "            else:\n",
    "                # Coalesce to try next format if previous failed\n",
    "                date_col = coalesce(date_col, temp_col)\n",
    "        \n",
    "        # Replace original column with parsed date\n",
    "        df = df.withColumn(col, date_col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text_fields(df: DataFrame, text_cols: list, id_cols: list) -> DataFrame:\n",
    "    \"\"\"Clean text and ID fields: trim spaces, normalize casing.\"\"\"\n",
    "    \n",
    "    # Clean text fields (trim and title case)\n",
    "    for col in text_cols:\n",
    "        df = df.withColumn(\n",
    "            col,\n",
    "            initcap(trim(col(col)))\n",
    "        )\n",
    "    \n",
    "    # Clean ID fields (trim and upper case)\n",
    "    for col in id_cols:\n",
    "        df = df.withColumn(\n",
    "            col,\n",
    "            upper(trim(col(col)))\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Add calculated columns: POAmount, OnTimePayment, AgingDays, Month.\"\"\"\n",
    "    \n",
    "    # Compute POAmount\n",
    "    df = df.withColumn(\"POAmount\", col(\"Quantity\") * col(\"UnitPrice_PO\"))\n",
    "    \n",
    "    # Define 'today' as the latest date from InvoiceDate or PaidDate\n",
    "    today = df.select(\n",
    "        greatest(\n",
    "            max(\"InvoiceDate\"),\n",
    "            max(\"PaidDate\")\n",
    "        ).alias(\"today\")\n",
    "    ).first()[\"today\"]\n",
    "    \n",
    "    # Create OnTimePayment column\n",
    "    df = df.withColumn(\n",
    "        \"OnTimePayment\",\n",
    "        when(\n",
    "            col(\"PaidDate\").isNotNull() & (col(\"PaidDate\") <= col(\"DueDate\")),\n",
    "            lit(True)\n",
    "        ).when(\n",
    "            col(\"PaidDate\").isNotNull() & (col(\"PaidDate\") > col(\"DueDate\")),\n",
    "            lit(False)\n",
    "        ).when(\n",
    "            col(\"PaidDate\").isNull() & (col(\"DueDate\") <= lit(today)),\n",
    "            lit(False)\n",
    "        ).otherwise(lit(None)).cast(BooleanType())\n",
    "    )\n",
    "    \n",
    "    # Create AgingDays column\n",
    "    df = df.withColumn(\n",
    "        \"AgingDays\",\n",
    "        when(\n",
    "            col(\"PaidDate\").isNull(),\n",
    "            datediff(lit(today), col(\"DueDate\"))\n",
    "        ).otherwise(\n",
    "            datediff(col(\"PaidDate\"), col(\"InvoiceDate\"))\n",
    "        ).cast(IntegerType())\n",
    "    )\n",
    "    \n",
    "    # Create Month column\n",
    "    df = df.withColumn(\n",
    "        \"Month\",\n",
    "        date_format(col(\"InvoiceDate\"), \"yyyy-MM\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE CREATION\n",
    "# ============================================================================\n",
    "# Execute the AP Invoices data transformation pipeline.\n",
    "\n",
    "print(f\"Loading source data from {INPUT_TABLE}...\")\n",
    "df = spark.table(INPUT_TABLE)\n",
    "\n",
    "print(\"Standardizing date formats...\")\n",
    "df = standardize_dates(df, DATE_COLS)\n",
    "\n",
    "print(\"Cleaning text and ID fields...\")\n",
    "df = clean_text_fields(df, TEXT_FIELDS, ID_FIELDS)\n",
    "\n",
    "print(\"Enriching data with calculated columns...\")\n",
    "df = enrich_data(df)\n",
    "\n",
    "print(f\"Writing enriched data to {OUTPUT_TABLE}...\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "print(\"Transformation pipeline completed successfully!\")\n",
    "\n",
    "\n",
    "# Display results\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7bcd73f-1362-43b2-a77e-fee8e89f9339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Por partes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c895835b-b338-486a-bd41-12fef98c8e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cambiar nombres de InvoiceDate a invoice_date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ded3e0-4a59-40d2-b2cd-6be771dd675d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ingest_time\":172},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764975687979}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Silver Layer - Accounts Payable Invoices Transformation Pipeline\n",
    "======================================================================\n",
    "This script performs data transformation on the 'ap.bronze.ap_invoices' table.\n",
    "\n",
    "Steps:\n",
    "    1. Standardize date formats\n",
    "    2. Clean text and ID fields\n",
    "    3. Create an enriched dataset with calculated fields:\n",
    "        - POAmount\n",
    "        - OnTimePayment\n",
    "        - AgingDays\n",
    "        - Month\n",
    "\n",
    "Assumptions:\n",
    "    - 'today' is the latest date found in 'invoice_date' or 'paid_date'.\n",
    "    - If 'paid_date' is blank, the invoice is considered unpaid.\n",
    "    - If unpaid and 'due_date' > 'today', the invoice is not considered overdue.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# DEPENDENCIES\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import BooleanType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "INPUT_TABLE = \"ap.bronze.ap_invoices\"\n",
    "OUTPUT_TABLE = \"ap.silver.ap_invoices_enriched\"  # Adjust as needed\n",
    "DATE_COLS = [\"invoice_date\", \"due_date\", \"paid_date\"]\n",
    "TEXT_FIELDS = [\"supplier\", \"category\"]\n",
    "ID_FIELDS = [\"invoice_id\", \"cost_center\", \"poid\"]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTIONS\n",
    "# ============================================================================\n",
    "def standardize_dates(df: DataFrame, columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert date columns to date type safely.\n",
    "    Tries multiple formats before falling back to default parsing.\n",
    "    \"\"\"\n",
    "    common_formats = [\n",
    "        \"MM/dd/yyyy\",  # 10/30/2025\n",
    "        \"MM/dd/yy\",    # 10/30/25\n",
    "        \"dd/MM/yyyy\",  # 30/10/2025\n",
    "        \"yyyy/MM/dd\",  # 2025/10/30\n",
    "        \"yyyy-MM-dd\"   # 2025-10-30\n",
    "    ]\n",
    "    \n",
    "    for col in columns:\n",
    "        # Try each format\n",
    "        date_col = None\n",
    "        for fmt in common_formats:\n",
    "            temp_col = to_date(col(col), fmt)\n",
    "            if date_col is None:\n",
    "                date_col = temp_col\n",
    "            else:\n",
    "                # Coalesce to try next format if previous failed\n",
    "                date_col = coalesce(date_col, temp_col)\n",
    "        \n",
    "        # Replace original column with parsed date\n",
    "        df = df.withColumn(col, date_col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text_fields(df: DataFrame, text_cols: list, id_cols: list) -> DataFrame:\n",
    "    \"\"\"Clean text and ID fields: trim spaces, normalize casing.\"\"\"\n",
    "    \n",
    "    # Clean text fields (trim and title case)\n",
    "    for col in text_cols:\n",
    "        df = df.withColumn(\n",
    "            col,\n",
    "            initcap(trim(col(col)))\n",
    "        )\n",
    "    \n",
    "    # Clean ID fields (trim and upper case)\n",
    "    for col in id_cols:\n",
    "        df = df.withColumn(\n",
    "            col,\n",
    "            upper(trim(col(col)))\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Add calculated columns: po_amount, on_time_payment, aging_days, month.\"\"\"\n",
    "    \n",
    "    # Compute POAmount\n",
    "    df = df.withColumn(\"po_amount\", col(\"quantity\") * col(\"unit_price_po\"))\n",
    "    \n",
    "    # Define 'today' as the latest date from invoice_date or paid_date\n",
    "    today = df.select(\n",
    "        greatest(\n",
    "            max(\"invoice_date\"),\n",
    "            max(\"paid_date\")\n",
    "        ).alias(\"today\")\n",
    "    ).first()[\"today\"]\n",
    "    \n",
    "    # Create OnTimePayment column\n",
    "    df = df.withColumn(\n",
    "        \"on_time_payment\",\n",
    "        when(\n",
    "            col(\"paid_date\").isNotNull() & (col(\"paid_date\") <= col(\"due_date\")),\n",
    "            lit(True)\n",
    "        ).when(\n",
    "            col(\"paid_date\").isNotNull() & (col(\"paid_date\") > col(\"due_date\")),\n",
    "            lit(False)\n",
    "        ).when(\n",
    "            col(\"paid_date\").isNull() & (col(\"due_date\") <= lit(today)),\n",
    "            lit(False)\n",
    "        ).otherwise(lit(None)).cast(BooleanType())\n",
    "    )\n",
    "    \n",
    "    # Create AgingDays column\n",
    "    df = df.withColumn(\n",
    "        \"AgingDays\",\n",
    "        when(\n",
    "            col(\"paid_date\").isNull(),\n",
    "            datediff(lit(today), col(\"due_date\"))\n",
    "        ).otherwise(\n",
    "            datediff(col(\"paid_date\"), col(\"invoice_date\"))\n",
    "        ).cast(IntegerType())\n",
    "    )\n",
    "    \n",
    "    # Create Month column\n",
    "    df = df.withColumn(\n",
    "        \"Month\",\n",
    "        date_format(col(\"invoice_date\"), \"yyyy-MM\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE CREATION\n",
    "# ============================================================================\n",
    "# Execute the AP Invoices data transformation pipeline.\n",
    "\n",
    "print(f\"Loading source data from {INPUT_TABLE}...\")\n",
    "df = spark.table(INPUT_TABLE)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9868e060-1253-47e2-ae5c-b49722060c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABLE CREATION\n",
    "# ============================================================================\n",
    "# Execute the AP Invoices data transformation pipeline.\n",
    "\n",
    "print(f\"Loading source data from {INPUT_TABLE}...\")\n",
    "df = spark.table(INPUT_TABLE)\n",
    "\n",
    "print(\"Standardizing date formats...\")\n",
    "df = standardize_dates(df, DATE_COLS)\n",
    "\n",
    "print(\"Cleaning text and ID fields...\")\n",
    "df = clean_text_fields(df, TEXT_FIELDS, ID_FIELDS)\n",
    "\n",
    "print(\"Enriching data with calculated columns...\")\n",
    "df = enrich_data(df)\n",
    "\n",
    "print(f\"Writing enriched data to {OUTPUT_TABLE}...\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "print(\"Transformation pipeline completed successfully!\")\n",
    "\n",
    "\n",
    "# Display results\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2648615b-65f9-4b47-8bdc-674d47d24655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "INPUT_TABLE = \"ap.bronze.ap_invoices\"\n",
    "OUTPUT_TABLE = \"ap.silver.ap_invoices_enriched\"  # Adjust as needed\n",
    "DATE_COLS = [\"invoice_date\", \"due_date\", \"paid_date\"]\n",
    "TEXT_FIELDS = [\"supplier\", \"category\"]\n",
    "ID_FIELDS = [\"invoice_id\", \"cost_center\", \"poid\"]\n",
    "\n",
    "\n",
    "def standardize_dates(df: DataFrame, columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert date columns to date type safely.\n",
    "    Tries multiple formats before falling back to default parsing.\n",
    "    \"\"\"\n",
    "    common_formats = [\n",
    "        \"MM/dd/yyyy\",  # 10/30/2025\n",
    "        \"MM/dd/yy\",    # 10/30/25\n",
    "        \"dd/MM/yyyy\",  # 30/10/2025\n",
    "        \"yyyy/MM/dd\",  # 2025/10/30\n",
    "        \"yyyy-MM-dd\"   # 2025-10-30\n",
    "    ]\n",
    "    \n",
    "    for c in columns:\n",
    "        # Try each format\n",
    "        date_col = None\n",
    "        for fmt in common_formats:\n",
    "            temp_col = try_to_date(col(c), fmt)\n",
    "            if date_col is None:\n",
    "                date_col = temp_col\n",
    "            else:\n",
    "                # Coalesce to try next format if previous failed\n",
    "                date_col = coalesce(date_col, temp_col)\n",
    "        \n",
    "        # Replace original column with parsed date\n",
    "        df = df.withColumn(c, date_col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(f\"Loading source data from {INPUT_TABLE}...\")\n",
    "df = spark.table(INPUT_TABLE)\n",
    "\n",
    "print(\"Standardizing date formats...\")\n",
    "df = standardize_dates(df, DATE_COLS)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c957c9be-3f77-45a0-8151-a8414b2db487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### UDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd0cab8-e661-4dcc-b9c0-28364730771f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"\n",
    "    Parse date string with multiple format attempts.\n",
    "    \"\"\"\n",
    "    if date_str is None or date_str.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    date_str = date_str.strip()\n",
    "    \n",
    "    # List of formats to try\n",
    "    formats = [\n",
    "        \"%m/%d/%Y\",  # 07/01/2025\n",
    "        \"%m/%d/%y\",  # 07/01/25\n",
    "        \"%-m/%d/%Y\", # 7/01/2025 (Unix-like systems)\n",
    "        \"%-m/%d/%y\", # 7/01/25\n",
    "        \"%-m/%-d/%Y\", # 7/1/2025\n",
    "        \"%-m/%-d/%y\", # 7/1/25\n",
    "        \"%Y-%m-%d\",  # 2025-07-01\n",
    "        \"%d/%m/%Y\",  # 01/07/2025\n",
    "    ]\n",
    "    \n",
    "    # Also try without leading zero removal (for Windows compatibility)\n",
    "    import re\n",
    "    # Remove leading zeros: \"07/01/25\" -> \"7/1/25\"\n",
    "    normalized = re.sub(r'\\b0(\\d)', r'\\1', date_str)\n",
    "    \n",
    "    for fmt in formats:\n",
    "        for test_str in [date_str, normalized]:\n",
    "            try:\n",
    "                # Remove %-  for Windows (doesn't support it)\n",
    "                fmt_clean = fmt.replace('%-', '%')\n",
    "                return datetime.strptime(test_str, fmt_clean).date()\n",
    "            except (ValueError, AttributeError):\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Register UDF\n",
    "parse_date_udf = udf(parse_date_flexible, DateType())\n",
    "\n",
    "def standardize_dates_udf(df, columns):\n",
    "    \"\"\"Use UDF for more flexible parsing\"\"\"\n",
    "    for col_name in columns:\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, parse_date_udf(col(col_name)))\n",
    "    return df\n",
    "\n",
    "print(f\"Loading source data from {INPUT_TABLE}...\")\n",
    "df = spark.table(INPUT_TABLE)\n",
    "\n",
    "print(\"Standardizing date formats with UDF...\")\n",
    "df = standardize_dates_udf(df, DATE_COLS)\n",
    "\n",
    "# Validation\n",
    "for date_col in DATE_COLS:\n",
    "    null_count = df.filter(col(date_col).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"Warning: {null_count} nulls in {date_col}\")\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transformation_ap_invoices_exp_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
