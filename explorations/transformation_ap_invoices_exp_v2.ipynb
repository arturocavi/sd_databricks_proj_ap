{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0efb992e-3183-4195-9514-a278ffd9ba18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transformations AP Invoices Exploration V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a303d66-1168-481b-8d04-036f8ef5b256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Silver Layer - Accounts Payable Invoices Transformation Pipeline\n",
    "======================================================================\n",
    "This script performs data transformation on the 'ap.bronze.ap_invoices' table.\n",
    "\n",
    "Steps:\n",
    "    1. Standardize date formats\n",
    "    2. Clean text and ID fields\n",
    "    3. Create an enriched dataset with calculated fields:\n",
    "        - POAmount\n",
    "        - OnTimePayment\n",
    "        - AgingDays\n",
    "        - Month\n",
    "\n",
    "Assumptions:\n",
    "    - 'today' is the latest date found in 'invoice_date' or 'paid_date'.\n",
    "    - If 'paid_date' is blank, the invoice is considered unpaid.\n",
    "    - If unpaid and 'due_date' > 'today', the invoice is not considered overdue.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# DEPENDENCIES\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import BooleanType, IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "INPUT_TABLE = \"ap.bronze.ap_invoices\"\n",
    "OUTPUT_TABLE = \"ap.silver.ap_invoices\"\n",
    "DATE_COLS = [\"invoice_date\", \"due_date\", \"paid_date\"]\n",
    "TEXT_FIELDS = [\"category\"]\n",
    "ID_FIELDS = [\"invoice_id\", \"cost_center\", \"poid\"]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTIONS\n",
    "# ============================================================================\n",
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"\n",
    "    Parse date string with multiple format attempts.\n",
    "    \"\"\"\n",
    "    if date_str is None or date_str.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    date_str = date_str.strip()\n",
    "    \n",
    "    # List of formats to try\n",
    "    formats = [\n",
    "        \"%m/%d/%Y\",  # 07/01/2025\n",
    "        \"%m/%d/%y\",  # 07/01/25\n",
    "        \"%-m/%d/%Y\", # 7/01/2025 (Unix-like systems)\n",
    "        \"%-m/%d/%y\", # 7/01/25\n",
    "        \"%-m/%-d/%Y\", # 7/1/2025\n",
    "        \"%-m/%-d/%y\", # 7/1/25\n",
    "        \"%Y-%m-%d\",  # 2025-07-01\n",
    "        \"%d/%m/%Y\",  # 01/07/2025\n",
    "    ]\n",
    "    \n",
    "    # Remove leading zeros: \"07/01/25\" -> \"7/1/25\"\n",
    "    normalized = re.sub(r'\\b0(\\d)', r'\\1', date_str)\n",
    "    \n",
    "    for fmt in formats:\n",
    "        for test_str in [date_str, normalized]:\n",
    "            try:\n",
    "                # Remove %-  for Windows (doesn't support it)\n",
    "                fmt_clean = fmt.replace('%-', '%')\n",
    "                return datetime.strptime(test_str, fmt_clean).date()\n",
    "            except (ValueError, AttributeError):\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Register UDF\n",
    "parse_date_udf = udf(parse_date_flexible, DateType())\n",
    "\n",
    "def standardize_dates_udf(df, columns):\n",
    "    \"\"\"Use UDF for more flexible parsing\"\"\"\n",
    "    for col_name in columns:\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, parse_date_udf(col(col_name)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text_fields(df: DataFrame, text_cols: list, id_cols: list) -> DataFrame:\n",
    "    \"\"\"Clean text and ID fields: trim spaces, normalize casing.\"\"\"\n",
    "    \n",
    "    # Clean text fields (trim and title case)\n",
    "    for col_name in text_cols:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            initcap(trim(col(col_name)))\n",
    "        )\n",
    "    \n",
    "    # Clean ID fields (trim and upper case)\n",
    "    for col_name in id_cols:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            upper(trim(col(col_name)))\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Add calculated columns: po_amount, on_time_payment, aging_days, month.\"\"\"\n",
    "    \n",
    "    # Compute POAmount\n",
    "    df = df.withColumn(\"po_amount\", col(\"quantity\") * col(\"unit_price_po\"))\n",
    "    \n",
    "    # Define 'today' as the latest date from invoice_date or paid_date\n",
    "    today = df.select(\n",
    "        greatest(\n",
    "            max(\"invoice_date\"),\n",
    "            max(\"paid_date\")\n",
    "        ).alias(\"today\")\n",
    "    ).first()[\"today\"]\n",
    "    \n",
    "    # Create OnTimePayment column\n",
    "    df = df.withColumn(\n",
    "        \"on_time_payment\",\n",
    "        when(\n",
    "            col(\"paid_date\").isNotNull() & (col(\"paid_date\") <= col(\"due_date\")),\n",
    "            lit(True)\n",
    "        ).when(\n",
    "            col(\"paid_date\").isNotNull() & (col(\"paid_date\") > col(\"due_date\")),\n",
    "            lit(False)\n",
    "        ).when(\n",
    "            col(\"paid_date\").isNull() & (col(\"due_date\") <= lit(today)),\n",
    "            lit(False)\n",
    "        ).otherwise(lit(None)).cast(BooleanType())\n",
    "    )\n",
    "    \n",
    "    # Create AgingDays column\n",
    "    df = df.withColumn(\n",
    "        \"aging_days\",\n",
    "        when(\n",
    "            col(\"paid_date\").isNull(),\n",
    "            datediff(lit(today), col(\"invoice_date\"))\n",
    "        ).otherwise(\n",
    "            datediff(col(\"paid_date\"), col(\"invoice_date\"))\n",
    "        ).cast(IntegerType())\n",
    "    )\n",
    "    \n",
    "    # Create Month column\n",
    "    df = df.withColumn(\n",
    "        \"Month\",\n",
    "        date_format(col(\"invoice_date\"), \"yyyy-MM\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_ingest_time(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Drop the 'ingest_time' column if it exists.\"\"\"\n",
    "    if \"ingest_time\" in df.columns:\n",
    "        return df.drop(\"ingest_time\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE CREATION\n",
    "# ============================================================================\n",
    "# Execute the AP Invoices data transformation pipeline.\n",
    "\n",
    "print(f\"Loading source data from {INPUT_TABLE}...\")\n",
    "df = spark.table(INPUT_TABLE)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720f776d-97cd-4671-9f98-4c94391d6ffe",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764981330614}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABLE CREATION\n",
    "# ============================================================================\n",
    "# Execute the AP Invoices data transformation pipeline.\n",
    "\n",
    "print(f\"Loading source data from {INPUT_TABLE}...\")\n",
    "df = spark.table(INPUT_TABLE)\n",
    "\n",
    "print(\"Standardizing date formats...\")\n",
    "df = standardize_dates_udf(df, DATE_COLS)\n",
    "\n",
    "print(\"Cleaning text and ID fields...\")\n",
    "df = clean_text_fields(df, TEXT_FIELDS, ID_FIELDS)\n",
    "\n",
    "print(\"Enriching data with calculated columns...\")\n",
    "df = enrich_data(df)\n",
    "\n",
    "print(\"Dropping 'ingest_time' column...\")\n",
    "df = drop_ingest_time(df)\n",
    "\n",
    "print(f\"Writing enriched data to {OUTPUT_TABLE}...\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "print(\"Transformation pipeline completed successfully!\")\n",
    "\n",
    "\n",
    "# Display results\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transformation_ap_invoices_exp_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
